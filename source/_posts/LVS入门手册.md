---
title: LVS入门手册
copyright: true
mathjax: true
categories:
  - 笔记
  - 运维
tags:
  - LVS
abbrlink: 17839
date: 2021-03-23 11:02:41
---

**LVS**是**Linux Virtual Server**的简写，意即**Linux虚拟服务器**，是一个虚拟的服务器集群系统。本项目在1998年5月由章文嵩博士成立，是中国国内最早出现的自由软件项目之一。

<!-- less -->



## LVS简介

**LVS**是**Linux Virtual Server**的简写，意即**Linux虚拟服务器**，是一个虚拟的服务器集群系统。本项目在1998年5月由章文嵩博士成立，是中国国内最早出现的自由软件项目之一。

LVS的主要为了解决大数据高并发的情况下的负载均衡和流量调度问题。

说起负载均衡，就不得不说Nginx，那么Nginx和LVS有哪些区别呢？



## LVS与Nginx的区别

### 作用

LVS：Linux虚拟机、流量调度、负载均衡

Nginx：高性能的代理服务器，系统内部流量分发，反向代理



### LVS的优势

- 抗负载能力强，因为 LVS 工作方式的逻辑是非常简单的，而且工作在网络的第 4 层，仅作请求分发用，没有流量，所以在效率上基本不需要太过考虑。LVS 一般很少出现故障，即使出现故障一般也是其他地方（如内存、CPU 等）出现问题导致 LVS 出现问题
- 配置性低，这通常是一大劣势同时也是一大优势，因为没有太多的可配置的选项，所以除了增减服务器，并不需要经常去触碰它，大大减少了人为出错的几率
- 工作稳定，因为其本身抗负载能力很强，所以稳定性高也是顺理成章的事，另外各种 LVS 都有完整的双机热备方案，所以一点不用担心均衡器本身会出什么问题，节点出现故障的话，LVS 会自动判别，所以系统整体是非常稳定的
- 无流量，LVS 仅仅分发请求，而流量并不从它本身出去，所以可以利用它这点来做一些线路分流之用。没有流量同时也保住了均衡器的 IO 性能不会受到大流量的影响
- LVS 基本上能支持所有应用，因为 LVS 工作在第 4 层，所以它可以对几乎所有应用做负载均衡，包括 http、数据库、聊天室等



### LVS与Nginx对比

- Nginx 工作在网络的第 7 层，可以作为网页静态服务器，支持 Rewrite 重写规则；支持 GZIP 压缩，节省带宽；可以做缓存；可以针对 http 应用本身来做分流策略，静态分离，针对域名、目录结构等
    相比之下 LVS 并不具备这样的功能，所以 Nginx 单凭这点可以利用的场合就远多于 LVS 了；但 Nginx 有用的这些功能使其可调整度要高于 LVS，所以经常要去触碰，人为出现问题的几率也就大
- Nginx 对网络的依赖较小，理论上只要 ping 得通，网页访问正常，Nginx 就能连得通，Nginx 同时还能区分内外网，如果是同时拥有内外网的节点，就相当于单机拥有了备份线路；LVS 就比较依赖于网络环境，目前来看服务器在同一网段内并且 LVS 使用 direct 方式分流，效果较能得到保证。另外注意，LVS 需要向托管商至少申请多于一个 ip 来做 visual ip(虚拟IP)

- Nginx 安装和配置比较简单，测试起来也很方便，因为它基本能把错误用日志打印出来。LVS 的安装和配置、测试就要花比较长的时间，因为同上所述，LVS 对网络依赖性比较大，很多时候不能配置成功都是因为网络问题而不是配置问题，出了问题要解决也相应的会麻烦的多
- Nginx 也同样能承受很高负载且稳定，但负载度和稳定度差 LVS 还有几个等级：Nginx 处理所有流量所以受限于机器 IO 和配置；本身的 bug 也还是难以避免的；Nginx 没有现成的双机热备方案，所以跑在单机上还是风险比较大，单机上的事情全都很难说
- Nginx 可以检测到服务器内部的故障（健康检查），比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点。目前 LVS 中 ldirectd 也能支持针对服务器内部的情况来监控，但 LVS 的原理使其不能重发请求。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx 会把上传切到另一台服务器重新处理，而 LVS 就直接断掉了



### 适用业务场景

- 网站建设初期（每日 PV 小于 100 万），可以选用 Nigix 作为反向代理负载均衡，因为其配置简单，性能也能满足一般的业务场景。如果考虑到负载均衡器是有单点问题，可以采用 Nginx + Keepalived 避免负载均衡器自身的单点问题
- 网站并发达到一定程度之后，为了提高稳定性和转发效率，可以使用 LVS、毕竟 LVS 比 Nginx 要更稳定，转发效率也更高。不过维护 LVS 对维护人员的要求也会更高，投入成本也更大



### LVS+Nginx 为什么会被同时使用

Nginx 用来做 http 的反向代理，能够 upsteam 实现 http 请求的多种方式的均衡转发。由于采用的是异步转发可以做到如果一个服务器请求失败，立即切换到其他服务器，直到请求成功或者最后一台服务器失败为止。这可以最大程度的提高系统的请求成功率

LVS 采用的是同步请求转发的策略。这里说一下同步转发和异步转发的区别。同步转发是在 LVS 服务器接收到请求之后，立即 redirect 到一个后端服务器，由客户端直接和后端服务器建立连接。异步转发是 Nginx 在保持客户端连接的同时，发起一个相同内容的新请求到后端，等后端返回结果后，由 Nginx 返回给客户端

进一步来说：当做为负载均衡服务器的 Nginx 和 LVS 处理相同的请求时，所有的请求和响应流量都会经过 Nginx；但是使用 LVS 时，仅请求流量经过 LVS 的网络，响应流量由后端服务器的网络返回

也就是，当作为后端的服务器规模庞大时，Nginx 的网络带宽就成了一个巨大的瓶颈

但是仅仅使用 LVS 作为负载均衡的话，一旦后端接受到请求的服务器出了问题，那么这次请求就失败了。但是如果在 LVS 的后端在添加一层 Nginx（多个），每个 Nginx 后端再有几台应用服务器，那么结合两者的优势，既能避免单 Nginx 的流量集中瓶颈，又能避免单 LVS 时一锤子买卖的问题

LVS + Nginx 的使用中，Nginx 还可以作为一个中间环节来减小后端 tomcat 的服务压力，以及做一些业务切换、分流、前置缓存的功能





## 搭建一个最基本的LVS测试服务

一下内容基于3台虚拟机中进行搭建：

LVS服务器 1台: 内部IP为 192.168.10.11

业务服务器2台： 内部IP为 192.168.10.12和192.168.10.13



**搭建过程：**

1. 在LVS服务器上进行设置

    ```shell
    # 该命令的作用为:在eth0 网卡下挂一个子网卡 ip地址为 192.168.10.100 子网掩码为 255.255.255.0
    # /24 等同于 netmask 255.255.255.0
    # 为什么等同,因为子网掩码分为4位,每一位的最大值为255,255的二进制编码为1111 1111,也就是8个1,那么
    # 3个255也就是24
    # ifconfig eth0:这个地方随便一个数字 该网段中的随便一个IP/24
    # 例如:
    ifconfig  eth0:8 192.168.10.100/24
    ```

2. 在具体业务服务器进行设置

    - 修改 服务器上内核协议

        ```powershell
        echo 1 > /proc/sys/net/ipv4/conf/eth0/arp_ignore
        echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore
         
        echo 2 > /proc/sys/net/ipv4/conf/eth0/arp_announce
        echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce
        ```

    - 在虚拟网卡上添加虚拟IP

        ```shell
        # ifconfig lo:这个地方随便一个数字 LVS的IP地址 netmask 255.255.255.255
        # 例如:
        ifconfig lo:8 192.168.10.155 netmask 255.255.255.255
        ```

    - 验证是否添加成功

        ```shell
        ifconfig
        ```

    - 安装httpd(该服务为一个静态server)

        ```shell
        # 在线安装httpd服务
        yum install httpd -y
        # 安装完成后进行启动
        service httpd start
        # 创建一个主页文件,输入 from 该服务器的IP,例如192.168.10.12
        # /var/www/html 为httpd的默认目录
        vi /var/www/html/index.html
        ```

3. 在LVS服务器上进行设置

    - 安装ipvsadm服务 

        ```shell
        yum install ipvsadm -y
        ```

    - 添加入包的规则

        ```shell
        # 语法:
        # 添加：-A -t|u|f service-address [-s scheduler]
        # -t: TCP协议的集群
        # -u: UDP协议的集群
        # service-address: IP:PORT
        # -f: FWM: 防火墙标记
        # service-address: Mark Number
        # 修改：-E
        # 删除：-D -t|u|f service-address
        
        # 调度算法(-s scheduler):
        # 静态调度算法： 
        # 1.rr（Round Robin）：轮询调度，轮叫调度。
        # 2.wrr（weight,加权）：以权重之间的比例实现在各主机之间进行调度。
        # 3.dh（Destination hashing）：目标地址散列。把同一个IP地址的请求，发送给同一个server。
        # 4.sh（source hashing）：源地址散列。主要实现会话绑定，能够将此前建立的session信息保留了。
        # 动态调度算法：
        # 1.lc（Least-Connection）: 最少连接
        # 2.wlc（Weighted Least-Connection Scheduling）: 加权最少连接
        # 3.sed（Shortest Expected Delay）: 最短期望延迟
        # 4.nq（never queue）: never queue
        # 5.LBLC（Locality-Based Least Connection）: 基于本地的最少连接
        # 6.LBLCR（Locality-Based Least Connections withReplication）: 基于本地的带复制功能的最少连接
        
        # 如果请求该服务器的请求为tcp协议，并且该请求的请求地址为192.168.10.100:80,那么则进行负载，负载的调度算法为轮训
        ipvsadm -A -t 192.168.10.100:80 -s rr
        ```

    - 查询是否添加成功

        ```shell
        ipvsadm -ln
        ```

    - 添加数据包的负载规则

        ```shell
        # 添加：-a -t|u|f service-address -r server-address [-g|i|m] [-w weight]
        #   -t|u|f service-address：事先定义好的某集群服务
        #   -r server-address: 某RS的地址，在NAT模型中，可使用IP：PORT实现端口映射；
        #   [-g|i|m]: LVS类型 
        #   -g: DR（修改链路层中的mac地址方式）
        #   -i: TUN（隧道方式）
        #   -m: NAT（修改网络层中的ip方式）
        #   [-w weight]: 定义服务器权重
        # 修改：-e
        # 删除：-d -t|u|f service-address -r server-address
        # ipvsadm -a -t 172.16.100.1:80 -r 192.168.10.8 –g
        # ipvsadm -a -t 172.16.100.1:80 -r 192.168.10.9 -g
        # 查看
        #   -L|l
        #   -n: 数字格式显示主机地址和端口
        #   --stats：统计数据
        #   --rate: 速率
        #   --timeout: 显示tcp、tcpfin和udp的会话超时时长
        #   -:c 显示当前的ipvs连接状况
        # 删除所有集群服务
        #   -C：清空ipvs规则
        # 保存规则
        #   -S
        # ipvsadm -S > /path/to/somefile
        # 载入此前的规则：
        #   -R
        # ipvsadm -R < /path/form/somefile 
        
        # 案例语法： ipvsadm -a -t 192.168.10.100:80 -r 192.168.10.12 -g -w 1
        # 解释： 将请求地址为 192.168.10.100:80 负载到 192.168.10.12 上，负载的类型为DR,权重为1
        
        # 案例：
        ipvsadm -a -t 192.168.10.100:80 -r 192.168.10.12 -g -w 1
        ipvsadm -a -t 192.168.10.100:80 -r 192.168.10.13 -g -w 1
        ```

    - 查询是否添加成功

        ```shell
        ipvsadm -ln
        ```

4. 验证是否设置成功

    - 在浏览器中访问192.168.10.100，如果能显示from 192.168.10.12或者13，那么重复刷新页面，如果能以轮训的方式展示12和13，那么代表设置成功

    - 在LVS服务器上，也就是设置192.168.10.100IP的服务器上输入命令

        ```shell
        # 该命令为查看该服务器上的连接
        netstat -natp 
        ```

    -  执行以上命令以后，会发现连接列表上并不存在 刚才一直访问的连接，那么得出一个结论，浏览器并没有与192.168.10.100服务器，也就是LVS服务器进行握手和挥手的操作。

    - 那么去192.168.10.12 和 192.168.10.13，也就是业务服务器中进行查看，会发现连接列表中存在连接

    - 也可以在LVS服务器上，输入`ipvsadm -lnc`查看LVS的偷窥记录





